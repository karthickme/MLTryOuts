{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather compilation project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Dayton - Environmental Protection Agency Average Daily Temperature Archive,\n",
    "http://academic.udayton.edu/kissock/http/Weather/default.htm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above link hosts average Daily temperature for many countries, Will start to explore and persome some analysis on the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Brief description of weather data and sources</h2>\n",
    "\n",
    "<p><br>\n",
    "This archive contains files of average daily temperatures for 157 <st1:country-region\n",
    "w:st=\"on\"><st1:place w:st=\"on\">U.S.</st1:place></st1:country-region> and 167 international\n",
    "cities. Source data for these files are from the Global Summary of the Day\n",
    "(GSOD) database archived by the National Climatic Data Center (NCDC).<span\n",
    "style='mso-spacerun:yes'>&nbsp; </span>The average daily temperatures posted on\n",
    "this site are computed from 24 hourly temperature readings in the Global\n",
    "Summary of the Day (GSOD) data.</p>\n",
    "\n",
    "<p>The data fields in each file posted on this site are: month, day, year,\n",
    "average daily temperature (F).<span style='mso-spacerun:yes'>&nbsp; </span>We\n",
    "use &quot;-99&quot; as a no-data flag when data are not available. </p>\n",
    "\n",
    "<div class=MsoNormal align=center style='text-align:center'>\n",
    "\n",
    "<hr size=2 width=\"100%\" align=center>\n",
    "\n",
    "</div>\n",
    "\n",
    "<p class=MsoNormal><o:p>&nbsp;</o:p></p>\n",
    "\n",
    "<h2>Comparison of &#8220;24 hour&#8221; and &#8220;(min + max) / 2&#8221;\n",
    "average daily temperatures</h2>\n",
    "\n",
    "<p class=MsoNormal><o:p>&nbsp;</o:p></p>\n",
    "\n",
    "<p class=MsoNormal>The average daily temperatures posted on this site are from\n",
    "Global Summary of the Day (GSOD) dataset and are computed from 24 hourly\n",
    "temperature readings. The GSOD dataset also includes daily minimum and maximum\n",
    "temperatures.<span style='mso-spacerun:yes'>&nbsp; </span>Some earlier datasets\n",
    "compiled by the NCDC, such as the Local <span class=SpellE>Climatological</span>\n",
    "Data Monthly Summary, contained daily minimum and maximum temperatures, but did\n",
    "not contain the average daily temperature computed from 24 hourly\n",
    "readings.<span style='mso-spacerun:yes'>&nbsp; </span>As a result, some users\n",
    "calculated the average daily temperature as the average of the daily minimum and\n",
    "maximum temperatures.</p>\n",
    "\n",
    "<p>We compared average daily temperatures calculated from 24 hourly readings,\n",
    "T24, to average daily temperatures calculated as the average of the daily\n",
    "minimum and maximum temperatures, <span class=SpellE>Tminmax</span>, for 53,004\n",
    "daily temperature records in the GSOD dataset.<span\n",
    "style='mso-spacerun:yes'>&nbsp; </span>We found that, on average, the absolute\n",
    "value of the deviation between T24 and <span class=SpellE>Tminmax</span> was\n",
    "1.48 F.<span style='mso-spacerun:yes'>&nbsp; </span>In addition, we found that,\n",
    "on average, <span class=SpellE>Tminmax</span> was 0.0790 F higher than T24.\n",
    "Temperatures in the GSOD dataset are reported with a precision of 0.1 F.<span\n",
    "style='mso-spacerun:yes'>&nbsp; </span>Thus, the average bias is less than the\n",
    "precision of the source data, and we conclude that the bias between T24 and <span\n",
    "class=SpellE>Tminmax</span> is not statistically significant.<span\n",
    "style='mso-spacerun:yes'>&nbsp; </span>If the bias is negligible, then the\n",
    "deviation is random and will sum to zero over any sufficiently long time\n",
    "period.<span style='mso-spacerun:yes'>&nbsp; </span>Thus, use of either T24 or <span\n",
    "class=SpellE>Tminmax</span> &#8220;average&#8221; daily temperatures should\n",
    "give similar results.<span style='mso-spacerun:yes'>&nbsp;&nbsp;&nbsp; </span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan at this point of time\n",
    "1. Analyse the page understand the page setup\n",
    "2. Then use bs4 to get the link download the contents and extract get a dataframe out of this.\n",
    "3. Then lets see how it progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting Non US data first\n",
    "url = 'https://academic.udayton.edu/kissock/http/Weather/citylistWorld.htm'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, features=\"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = soup.find_all(\"li\",class_=\"MsoNormal\")\n",
    "# base_url = \"https://academic.udayton.edu/kissock/http/Weather/\"  # commenting as the way data pull is changed.\n",
    "all_cities = []\n",
    "# all_links = []\n",
    "all_filename = []\n",
    "all_countries = []\n",
    "for city in cities:\n",
    "    city_name = city.text.split(\"(\")[0].rstrip()\n",
    "    all_cities.append(city_name)\n",
    "    # link = city.find('a')['href']    # commenting as the way data pull is changed.\n",
    "    filename = city.find('a')['href'].split('/')[-1]\n",
    "    all_filename.append(filename)\n",
    "    # page is having multiple a links in each case but the city is presendin in just 1 a tag so did some stupid workaround which works!!\n",
    "    country = city.find_all(\"a\")[-1].text[:2].strip() + city.find_all(\"a\")[0].text[:2].strip() \n",
    "    # all_links.append(base_url + link)   # commenting as the way data pull is changed.\n",
    "    all_countries.append(country[:2])\n",
    "# print(len(all_cities))\n",
    "# print(len(all_filename))\n",
    "# print(len(all_countries))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change of plan\n",
    "\n",
    "\n",
    "Got to know a single place to download the data but there are some conuntry code and US City code conflict. so preparing them 1ft\n",
    "- So going with only non US Cities first < br >\n",
    "- One that is completed then will work on US cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting US data\n",
    "url = 'https://academic.udayton.edu/kissock/http/Weather/citylistUS.htm'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, features=\"lxml\")\n",
    "cities = soup.find_all(\"li\", class_=\"MsoNormal\")\n",
    "all_UScities = []\n",
    "all_USfilename = []\n",
    "all_USstate = []\n",
    "for city in cities:\n",
    "    city_name = city.text.split(\"(\")[0].rstrip()\n",
    "    all_UScities.append(city_name)\n",
    "    # link = city.find('a')['href']    # commenting as the way data pull is changed.\n",
    "    filename = city.find('a')['href'].split('/')[-1]\n",
    "    all_USfilename.append(filename)\n",
    "    # page is having multiple a links in each case but the city is presendin in just 1 a tag so did some stupid workaround which works!!\n",
    "    state = city.find_all(\"a\")[-1].text[:2].strip() + \\\n",
    "        city.find_all(\"a\")[0].text[:2].strip()\n",
    "    # all_links.append(base_url + link)   # commenting as the way data pull is changed.\n",
    "    all_USstate.append(state[:2])\n",
    "print(len(all_UScities))\n",
    "print(len(all_USfilename))\n",
    "print(len(all_USstate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Site is slow, So sending 300+ requests(Mulitple time when testing does not seems to fair) will be huge so planned to download the entire data use the file name to read from file specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So steps to do now..\n",
    "\n",
    "1. Download the entire file with all the data\n",
    "2. Unzip the file to a data directory\n",
    "3. fileformat \n",
    "   1. US Data --> SSCCCCCC.txt (SS-State abbreviation CCCCCC-City abbreviation up to 6 letters). \n",
    "   2. Internatinal data -->  SSCCCCCC.txt (SS-Country abbreviation CCCCCC-City abbreviation up to 6 letters).\n",
    "4. create two dataframe one for US and other international with datetime series as index, colums will each of the city \n",
    "5. Get country code - counntry reff and US state code and States name from a relaible source for plotting\n",
    "6. Then begin with plotting to see if there are any pattern\n",
    "7. other steps will be determined futher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the requests module\n",
    "import requests\n",
    "url = \"https://academic.udayton.edu/kissock/http/Weather/gsod95-current/allsites.zip\"\n",
    "\n",
    "if os.path.exists('allsites.zip'):\n",
    "    print(\"file Already present\")\n",
    "else:\n",
    "    print('Downloading started')\n",
    "    # Downloading the file by sending the request to the URL\n",
    "    req = requests.get(url)\n",
    "\n",
    "    # Split URL to get the file name\n",
    "    filename = url.split('/')[-1]\n",
    "\n",
    "    # Writing the file to the local file system\n",
    "    with open(filename, 'wb') as output_file:\n",
    "        output_file.write(req.content)\n",
    "    print('Downloading Completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "filename = \"allsites.zip\"\n",
    "print(filename)\n",
    "try:\n",
    "    with zipfile.ZipFile(filename) as z:\n",
    "        z.extractall('data/03Temperature')\n",
    "        print(\"Extracted all\")\n",
    "except:\n",
    "    print(\"Invalid file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# County code https://www.iban.com/country-codes\n",
    "import pandas as pd\n",
    "url = \"https://www.iban.com/country-codes\"\n",
    "\n",
    "country_code = pd.read_html(url)[0]\n",
    "\n",
    "country_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleanup of the read txt file\n",
    "\n",
    "Below function createcsv will read the txt files extracted and coverets to a CSV, with correct date foramt and the temperature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def createcsv(path):\n",
    "    \"\"\"Received one parameter, convers the these files to a proper CSV! \"\"\"\n",
    "    templist = []\n",
    "    file = open(path, \"r\")\n",
    "    if path[-3:] == \"LOG\":\n",
    "        return 0\n",
    "    print(f\"started {path}\", end=\" **\")\n",
    "    while(True):\n",
    "        # read next line\n",
    "        line = file.readline()\n",
    "        # check if line is not null\n",
    "        if not line or line.strip() == \"\":\n",
    "            break\n",
    "        numbers = re.findall('[-.0-9]+', line)\n",
    "        if int(numbers[1]) == 0:  # Dirty xix to address a erorr date issue in CUHAVANA.txt\n",
    "            # Dirty xix to address a erorr date issue in CUHAVANA.txt and DLHAMBUR.txt\n",
    "            numbers[1] = 29\n",
    "        if int(numbers[2]) == 201:\n",
    "            continue\n",
    "        # print(numbers)\n",
    "        col1 = datetime.date(\n",
    "            int(numbers[2]), int(numbers[0]), int(numbers[1]))\n",
    "        col2 = float(numbers[-1])\n",
    "        # print(col1, col2)\n",
    "        templist.append([col1,col2])\n",
    "    # print(templist[:5])\n",
    "    print(end=\" ** \")\n",
    "    tempdf = pd.DataFrame(templist,columns=['date', 'temp'])\n",
    "    # print(path[:-3]+\"csv\")\n",
    "    # tempdf.to_csv(\"00_deleteme.csv\",index=False)\n",
    "    tempdf.to_csv(path[:-3]+\"csv\", index=False)\n",
    "    print(end=\"**\")\n",
    "    print(end=\"**\")\n",
    "    file.close()\n",
    "    print(end=\" ** \")\n",
    "    os.remove(path)\n",
    "    print(\" done with \" + path[19:-3]+\"csv\")\n",
    "\n",
    "\n",
    "# createcsv('data/03Temperature/ABTIRANA.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the os library\n",
    "import os\n",
    "# The path for listing items\n",
    "path = 'data/03Temperature'\n",
    "# The list of items\n",
    "files = os.listdir(path)\n",
    "# Loop to print each filename separately\n",
    "for filename in files:\n",
    "    full_path = path+'/'+filename\n",
    "    createcsv(full_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below this point -- Work still going on!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = 'data/03Temperature'\n",
    "# The list of items\n",
    "files = os.listdir(path)\n",
    "# Loop to print each filename separately\n",
    "all_csvs = [path+'/'+filename for filename in files]\n",
    "print(len(all_csvs))\n",
    "\n",
    "df_nonUS = pd.read_csv(all_csvs.pop(), index_col=\"date\")\n",
    "print(df_nonUS.head())\n",
    "print(len(all_csvs))\n",
    "\n",
    "# left = pd.DataFrame({\n",
    "#     'Sr': [6, 7, 8, 9, 2],\n",
    "#     'Name': ['Span', 'Suchu', 'Vetts', 'Appu', 'Sri']})\n",
    "# right = pd.DataFrame({\n",
    "#     'Sr': [6, 7, 8, 9, 20],\n",
    "#     'Name': ['fil', 'mil', 'sil', 'pil', 'gil']})\n",
    "# print(pd.merge(left, right, on='Sr', how='outer'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sr</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name\n",
       "Sr     \n",
       "6   fil\n",
       "7   mil\n",
       "8   sil\n",
       "9   pil\n",
       "20  gil"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "left = pd.DataFrame({\n",
    "    'Sr': [6, 7, 8, 9, 2],\n",
    "    'Name': ['Span', 'Suchu', 'Vetts', 'Appu', 'Sri']}) #, index='Sr')\n",
    "left.set_index('Sr')\n",
    "right = pd.DataFrame({\n",
    "    'Sr': [6, 7, 8, 9, 20],\n",
    "    'Name': ['fil', 'mil', 'sil', 'pil', 'gil']}) #,index='Sr')\n",
    "right.set_index('Sr')\n",
    "# print(pd.merge(left, right, on='Sr', how='outer'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr_x</th>\n",
       "      <th>Name_x</th>\n",
       "      <th>Sr_y</th>\n",
       "      <th>Name_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Span</td>\n",
       "      <td>6</td>\n",
       "      <td>fil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Suchu</td>\n",
       "      <td>7</td>\n",
       "      <td>mil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Vetts</td>\n",
       "      <td>8</td>\n",
       "      <td>sil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Appu</td>\n",
       "      <td>9</td>\n",
       "      <td>pil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Sri</td>\n",
       "      <td>20</td>\n",
       "      <td>gil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr_x Name_x  Sr_y Name_y\n",
       "0     6   Span     6    fil\n",
       "1     7  Suchu     7    mil\n",
       "2     8  Vetts     8    sil\n",
       "3     9   Appu     9    pil\n",
       "4     2    Sri    20    gil"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(left, right, left_index=True, right_index=True)\n",
    "left.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4831faad42022cb3b0932f3bf965c0323cdc750e5e185dff2cc12c78d9e2dc2b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('MLTryOuts-SDuJ_iQf': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
